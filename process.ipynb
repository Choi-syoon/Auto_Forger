{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import speech_recognition as sr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import Model, Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import tokenizer_from_json, Tokenizer\n",
    "from tensorflow.python.keras.layers import Dense, RNN, Embedding, GlobalAveragePooling1D\n",
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences  - from tf version 2.6.0\n",
    "# from tensorflow.python.keras.preprocessing.text import tokenizer_from_json, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # 데이터셋 정보 json 탐색 함수\n",
    "    def search_json(self, dir):\n",
    "        paths = []\n",
    "        count = 0\n",
    "        for data in os.listdir(dir):\n",
    "            file_path = os.path.join(dir, data)\n",
    "            if os.path.isdir(file_path):\n",
    "                count += self.search_json(file_path)\n",
    "            elif data.endswith('.json'):\n",
    "                count += 1\n",
    "                paths.append(file_path)\n",
    "        print(paths)\n",
    "        return count\n",
    "    \n",
    "    # 데이터셋 발화자 id 확인 함수 - Use Text\n",
    "    def id_checker(self, json_path):\n",
    "        id_sector = []\n",
    "        with open(json_path, encoding='utf-8-sig') as file:\n",
    "            json_data = json.load(file) # load json\n",
    "            for i in range(0, 2):   # id search loop\n",
    "                id = json_data['dataSet']['dialogs'][i]['speaker']  # id 0,1 indexing\n",
    "                category = json_data['dataSet']['typeInfo']['category'] # category indexing\n",
    "                if id and category is not None:\n",
    "                    id_sector.append(id)\n",
    "            v1, v2 = id_sector[0], id_sector[1]\n",
    "        print(f\"Server : {v1}\\nClient : {v2}\")\n",
    "        print(f\"Category : {category}\")\n",
    "        return v1, v2, category\n",
    "\n",
    "    # audio to text - stt module\n",
    "    def stt_from_array(audio_data):\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(audio_data) as source:\n",
    "            audio = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"stt : {text}\")\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Exception : UnknownValueError\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Exception : STT engine RequestError\")\\\n",
    "            \n",
    "    \n",
    "    \n",
    "    # def integer_indexer():///\n",
    "    \n",
    "    # def data_generator():\n",
    "\n",
    "    # def data_loader():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenProcessor:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.word_set = []\n",
    "\n",
    "    def tokenize_index(self, input_texts):\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        tokenized_texts = tokenizer.tokenize(' '.join(input_texts))\n",
    "\n",
    "        word_list = [word.lower() for word in tokenized_texts if word.isalpha()]\n",
    "        word_freq = FreqDist(word_list)\n",
    "\n",
    "        sorted_words = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "        word_to_index = {word: index + 1 for index, word in enumerate(sorted_words)}\n",
    "        indexed_texts = [word_to_index[word.lower()] for word in tokenized_texts if word.isalpha()]\n",
    "\n",
    "        print(f\"Detect Language: {detect(' '.join(input_texts))}\")\n",
    "        print(f\"Input Texts: {input_texts}\")\n",
    "        print(f\"Tokenized Sequences: {tokenized_texts}\")\n",
    "        print(f\"Indexed Sequences: {indexed_texts}\")\n",
    "        print(f\"Word to Index : {word_to_index}\")\n",
    "\n",
    "        return indexed_texts\n",
    "    \n",
    "    def stopwords_slicer(self, input_texts):\n",
    "        pattern = r'\\s*n/|\\(.*\\)'\n",
    "        sliced_text = re.sub(pattern, ' ', input_texts)\n",
    "        return sliced_text\n",
    "    \n",
    "    def read_talk_data(self):   # 데이터셋 text 파일 (read, return)\n",
    "        contents = []\n",
    "        for filename in os.listdir(self.path):\n",
    "            if filename.endswith('.txt'):\n",
    "                filePath = os.path.join(self.path, filename)\n",
    "                with open(filePath, 'r') as text:\n",
    "                    file_contents = text.read()\n",
    "                    cleaned = self.stopwords_slicer(file_contents)\n",
    "                    contents.append(cleaned)\n",
    "        return contents\n",
    "    \n",
    "    def pad_seqs(self, sequences): \n",
    "        padded_seq = pad_sequences(sequences)\n",
    "        return padded_seq\n",
    "            \n",
    "    def run(self):  # main worker\n",
    "        contents = self.read_talk_data()\n",
    "        print(contents)\n",
    "        # indexed_texts = self.tokenize_index(contents)\n",
    "        # return indexed_texts\n",
    "        # padded_seqs = self.pad_seqs([indexed_texts])\n",
    "        # print(f\"Padded_seqs shape : {padded_seqs.shape}\")\n",
    "        # return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './local/sample/label/S00007727/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  안녕하십니까, 고객님.', '  네, 우리 아이 학습지 좀 새로 등록해볼까  .', '  네, 아이 나이가 어떻게 되는지 여쭤봐도 될까요?', '   .', '  네, 미취학 아동을 대상으로 한 프로그램이 준비되어있습니다.', '  그런데 우리 아이가 말을 늦게 배운 편이라서, 단계가 여러 개면 좀 낮은 걸로 먼저 들어+ 들어야 될 것 같은데요. 홈페이지 보니까 프로그램이 여러 개고 그 안에서 단계도 여러 개 나누어져 있던데 미취학 아동 프로그램은 단계가 총 몇 개로 나누어져 있죠?', '  저희가 신규 등록 시에 무료로 레벨 테스트를 하고 있습니다.', '  아/ 그래요? 레벨 테스트를 어떻게 하는데요? 레벨 테스트 후에 단계를 정해주시는  ?', '  방문 교사가 직접 방문드린 뒤에 저희 쪽 교재를 가지고 아이의 발달 사항을 체크한 후에 단계를 정해드립니다.', '  애가 낯을 많이 가려서 그런데 제가 옆에서 같이 있어도 되나요? 애가 낯을 가리긴 하는데 좀 산만한 편이에요.', '  네, 가능하십니다.', '  그러면 레벨 테스트를 우선 받아볼게요. 레벨 테스트는 무료인거면 레벨테스트만 받고 등록 안 해도 무료인 거죠?', '  네, 그렇습니다.', '  네, 그럼 일단 등록은 안 하고 레벨테스트만 들어볼게요. 홈페이지에 프로그램 내용은 정확하게 안 나와있던데 미취학 아동 프로그램 안에서 또 프로그램이 나눠져 있는 거 있는 거죠? 아니면 미취학 아동 프로그램은 딱  ?', '  저희 프로그램이 크게 한글 프로그램이랑 영어 프로그램으로 나누어져 있는데 어떤 프로그램을 찾으세요?', '  음/ 원래는 한글을 가르치려고 했는데 영어도 같이 하면 좀 저렴한가요? 저렴하면 영어랑   받을 수 있어요?', '  프로그램  까지는 따로 할인되는 게 없으세요.', '    할인된다고 적혀있던데. 이건 그럼 뭔데요?', '  카드사 할인이십니다. 고객님.', '  그럼 카드사 할인이라고 미리 적어놨어야죠. 홈페이지만 보면 카드사 할인인지 그냥 할인인지 모르잖아요. 저희는 체크카드만 써서 따로 할인받을 수 있는 게 없어요. 저희 아이랑 같은 유치원 다니는 친구는 학습지 할인 받아서 하고 있다던데 조금만 할인해주세요.', '  카드사 할인 이외에는 패키지 상품 할인이 있으신데 이건 어떠세요?', '  패키지 상품 할인이면 한글이랑 영어 들었을 때도 적용되는 거 아니에요?', '  패키지 상품은 한글, 영어, 논술을 같이 들으시는 상품입니다.', '   . 웬만하면 그냥 해주세요. 레벨 테스트 받아보고 괜찮으면 오래 배울 생각이에요.', '  그러시면 할인이 어려울 것 같습니다. 죄송합니다.', '  그러면 일단 레벨 테스트 받아보고 결정을 해볼 테니 일단 논술 레벨 테스트도 같이 신청해주세요. 이렇게 레벨테스트   받더라도 레벨 테스트니까 무료인 거죠? 이번주밖에 시간 안 되는데 그럼 빨리 받을 수 있게 얼른 접수해주세요.', '  네, 무료입니다. 접수해드리겠습니다.', '  그리고 레벨 테스트를 받고 학습지 등록하게 되면 주 몇 회 수업인  인가요?', '  네, 그렇습니다.', '  금액이  밖에 안 해줘요? 무슨 미취학 대상 아동 프로그램을 이렇게 비싸게 받나요? 꼬맹이들 가르치는 거라 선생님들도 별로 안 힘들 것 같은데.', '  교구 값이 포함된 금액입니다.', '  그러면 좀 들을만하네요. 그럼 두 개+  까진 어려울 것 같은데 방법 없나요?', '  아이 체력이 가능하다면 주  에 몰아서 수업 받으실 수도 있어요.', '  제가 옆에 있어야 해서 차라리 그게 낫겠네요. 일단 레벨 테스트는 하루에 다 몰아서 받게 해 주시고, 그 이후에 등록하게 되면 나눠서 받던가 할게요.', '  한글이랑 영어만 받게 되면 주   수업에 몇 분 정도인 거죠?', '   입니다.', '  미취학 아동들이 보통  이나 붙어있을 수는 없는데. 그럼 방금 상담 기록 남긴 거 지워주시고 그냥 한글이랑 영어만 일단 레벨 테스트 해볼게요.', '  논술은 안 듣는 걸로  이나 붙어있어야 해서 저도 힘들 것 같네요. 그리고 논술 레벨 테스트는 그냥 취소할게요.', '  네, 알겠습니다.', '  그리고 홈페이지 보니까 등록 이벤트 하고 있어서 등록하면 교육용 라디오 주던데 그거 저도 받을 수 있는 거죠?', '  상담과 동시에 등록하셔야 증정되는 상품입니다.', '  아니, 교육용 라디오 그거 얼마 한다고 그걸 오늘 등록해야 주는   가져다주세요.', '  그 정도 유도리는 있어야죠. 그렇게 해주시면 저도 홍보 많이 할게요.', '  그건 제가 결정할 수 있는 부분이 아니라서 담당자님께 보고 드리겠습니다.', '  네, 최대한 좀 부탁드릴게요. 그럼 이번 주는 제가 쉬는 주니까 아침이나 저녁만 아니면 아무 때나 오셔도 될 것 같아요. 꼭 이번 주에 레벨테스트 먼저 받을 수 있도록 해주세요.', '  네, 알겠습니다.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenProcessor(path)\n",
    "indexed_texts = tokenizer.run()\n",
    "# padded_seqs = tokenizer.run()\n",
    "# print(padded_seqs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
